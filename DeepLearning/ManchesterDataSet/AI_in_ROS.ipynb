{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8W44vyaZUcd"
      },
      "outputs": [],
      "source": [
        "# Main requirements \n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.utils.np_utils import to_categorical\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "import pandas as pd\n",
        "import random\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTyDk_q9a7qq"
      },
      "source": [
        "BEG FUNTION DEF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a55HfuchbBlJ"
      },
      "outputs": [],
      "source": [
        "def grayscale(img):\n",
        "    img = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
        "    return img\n",
        "def equalize(img):\n",
        "    img =cv2.equalizeHist(img)\n",
        "    return img\n",
        "def preprocessing(img):\n",
        "    img = grayscale(img)     # CONVERT TO GRAYSCALE\n",
        "    img = equalize(img)      # STANDARDIZE THE LIGHTING IN AN IMAGE\n",
        "    img = img/255            # TO NORMALIZE VALUES BETWEEN 0 AND 1 INSTEAD OF 0 TO 255\n",
        "    return img\n",
        "\n",
        "#Reshape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uap38SeXbVU8"
      },
      "outputs": [],
      "source": [
        "def myModel():\n",
        "    # Hyperparameter selection                                                                                                                                                                                                                   \n",
        "    # Filters of the CNN                                                                                                                                                                                                         \n",
        "    no_Of_Filters=60                                                                                                                                                                                                             \n",
        "    # Shape of the filters used in the CNN                                                                                                                                                                                       \n",
        "    size_of_Filter=(5,5)                                                                                                                                                                                                         \n",
        "    size_of_Filter2=(3,3)                                                                                                                                                                                                        \n",
        "    # Tekes batches of 2x2 pixels and avg the                                                                                                                                                                                    \n",
        "    size_of_pool=(2,2)                                                                                                                                                                                                           \n",
        "    # Nodes of the neural classifier                                                                                                                                                                                             \n",
        "    no_Of_Nodes = 500                                                                                                                                                                                                            \n",
        "                                                                                                                                                                                                                                 \n",
        "    # TODO: Add layers as presented in the class to conform your CNN                                                                                                                                                                                                                              \n",
        "                                                                                                                                                                           \n",
        "    model.compile(optimizer=tf.keras.optimizers.RMSprop(),                                                                                                                                                                       \n",
        "              loss=tf.keras.losses.BinaryCrossentropy(),                                                                                                                                                                         \n",
        "              metrics=[tf.keras.metrics.BinaryAccuracy(),                                                                                                                                                                        \n",
        "                       tf.keras.metrics.FalseNegatives()])                                                                                                                                                                       \n",
        "                                                                                                                                                                                                                                 \n",
        "    return model                                                                                                                                                                                                                 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oG2ZROdFa-6p"
      },
      "source": [
        "\n",
        "\n",
        "END FUNTION DEF\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1r_cr0nHmclS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c747c3ba-c369-4c02-bb23-36835e7e731c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmxNwbREagzL"
      },
      "outputs": [],
      "source": [
        "################# Parameters #####################\n",
        "tf.config.set_visible_devices([], 'GPU')\n",
        "\n",
        "path = \"drive/Shareddrives/TE3002B/AI_in_ROS/MyData\" # folder with all the class folders\n",
        "labelFile = 'drive/Shareddrives/TE3002B/AI_in_ROS/labels.csv' # file with all names of classes\n",
        "batch_size_val=50  # how many to process together before updating the interanl parameters\n",
        "steps_per_epoch_val=100 # we divide all our database in 10 bathces \n",
        "epochs_val=10\n",
        "imageDimesions = (32,32,3)\n",
        "testRatio = 0.2    # if 1000 images split will 200 for testing\n",
        "validationRatio = 0.2 # if 1000 images 20% of remaining 800 will be 160 for validation\n",
        "###################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZUYQZ8bai10"
      },
      "outputs": [],
      "source": [
        "############################### Importing of the Images\n",
        "count = 0\n",
        "images = []\n",
        "classNo = []\n",
        "myList = os.listdir(path)\n",
        "print(\"Total Classes Detected:\",len(myList))\n",
        "noOfClasses=len(myList)\n",
        "print(\"Importing Classes.....\")\n",
        "\n",
        "#Import names\n",
        "for x in range (0,len(myList)):\n",
        "    myPicList = os.listdir(path+\"/\"+str(count))\n",
        "    for y in myPicList:\n",
        "        curImg = cv2.imread(path+\"/\"+str(count)+\"/\"+y)\n",
        "        images.append(curImg)\n",
        "        classNo.append(count)\n",
        "    print(\"{0} of {1}\".format(count, len(myList)), end =\"\\n\")\n",
        "    count +=1\n",
        "print(\" \")\n",
        "images = np.array(images)\n",
        "classNo = np.array(classNo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OzgYO4j7al5Z"
      },
      "outputs": [],
      "source": [
        "############################### Split Data\n",
        "X_train, X_test, y_train, y_test = train_test_split(images, classNo, test_size=testRatio)\n",
        "X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=validationRatio)\n",
        " \n",
        "# X_train = ARRAY OF IMAGES TO TRAIN\n",
        "# y_train = CORRESPONDING CLASS ID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PsP8ythkas1p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8e807ad-62ac-4df8-bd18-f38788814fa0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data shape  (43, 2) <class 'pandas.core.frame.DataFrame'>\n",
            "17076\n",
            "5337\n",
            "43 2\n"
          ]
        }
      ],
      "source": [
        "############################### READ CSV FILE\n",
        "data=pd.read_csv(labelFile)\n",
        "print(\"data shape \",data.shape,type(data))\n",
        "\n",
        "###################################################3LF\n",
        "print(len(X_train))\n",
        "print(len(X_test))\n",
        "#print(len(label_file), label_file.shape)\n",
        "#print(label_file[:10])\n",
        "#####\n",
        "H,W=data.shape\n",
        "print(H,W)\n",
        "#data = data.reshape((17076, 55 * 57 *3))\n",
        "#data = data.astype('float32') / 255)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-53lOEjlavpG"
      },
      "outputs": [],
      "source": [
        "############################### DISPLAY SOME SAMPLES IMAGES  OF ALL THE CLASSES\n",
        "num_of_samples = []\n",
        "cols = 5\n",
        "num_classes = noOfClasses\n",
        "fig, axs = plt.subplots(nrows=num_classes, ncols=cols, figsize=(5, 300))\n",
        "fig.tight_layout()\n",
        "for i in range(cols):\n",
        "    for j,row in data.iterrows():\n",
        "        x_selected = X_train[y_train == j]\n",
        "        axs[j][i].imshow(x_selected[random.randint(0, len(x_selected)- 1)], cmap=plt.get_cmap(\"gray\"))\n",
        "        axs[j][i].axis(\"off\")\n",
        "        if i == 2:\n",
        "            axs[j][i].set_title(str(j)+ \"-\"+row[\"Name\"])\n",
        "            num_of_samples.append(len(x_selected))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqVzTDkbaxOl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "7e92c123-4a07-4830-d1a5-ab7ff8a2d88b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[105, 965, 946, 625, 840, 819, 190, 612, 604, 644, 846, 554, 894, 933, 353, 263, 200, 481, 499, 102, 143, 146, 164, 237, 114, 643, 266, 119, 236, 114, 194, 357, 125, 314, 192, 534, 171, 101, 905, 137, 153, 116, 120]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "############################### DISPLAY A BAR CHART SHOWING NO OF SAMPLES FOR EACH CATEGORY\n",
        "print(num_of_samples)\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.bar(range(0, num_classes), num_of_samples)\n",
        "plt.title(\"Distribution of the training dataset\")\n",
        "plt.xlabel(\"Class number\")\n",
        "plt.ylabel(\"Number of images\")\n",
        "plt.show()\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(images, classNo, test_size=testRatio)\n",
        "X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=validationRatio)"
      ],
      "metadata": {
        "id": "QXPNW2TrJ7kN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YCArJGJluwZQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b1cd264-3dab-424c-bdc5-0dd1d0add931"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before\n",
            "Len X_train:  17076\n",
            "X_train[1]:  (44, 43, 3)\n",
            "X_train shape: (17076,)\n",
            "y_train shape: (17076,)\n",
            "X_test shape: (5337,)\n",
            "y_test shape: (5337,)\n",
            "#####################################################\n",
            "#####################################################\n",
            "Tipo1 <class 'numpy.ndarray'>\n",
            "Tipo1 <class 'numpy.ndarray'>\n",
            "After\n",
            "X_trainp:  17049\n",
            "y_trainp:  17049\n",
            "X_testp:  5328\n",
            "y_testp:  5328\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.core.framework.types_pb2 import DataType\n",
        "############################### Split Data\n",
        "\n",
        " \n",
        "# X_train = ARRAY OF IMAGES TO TRAIN\n",
        "# y_train = CORRESPONDING CLASS ID\n",
        "\n",
        "# TODO: Adapt the X_train, X_test, X_validation, y_train, y_test and y_validation arrays for the proper neural network training, validation and testing\n",
        "#lf\n",
        "\n",
        "\n",
        "print(\"Before\")\n",
        "print(\"Len X_train: \",len(X_train))\n",
        "print(\"X_train[1]: \",X_train[1].shape)\n",
        "#print(\"y_train[1]: \",y_train.shape)\n",
        "print(f'X_train shape: {X_train.shape}')\n",
        "print(f'y_train shape: {y_train.shape}')\n",
        "print(f'X_test shape: {X_test.shape}')\n",
        "print(f'y_test shape: {y_test.shape}')\n",
        "\n",
        "print(\"#####################################################\")\n",
        "\n",
        "X_trainp=[]\n",
        "X_testp=[]\n",
        "y_trainp=[]\n",
        "y_testp=[]\n",
        "\n",
        "print(\"#####################################################\")\n",
        "\n",
        "tipo=type(X_train[1])\n",
        "tipo2=type(X_test[1])\n",
        "print(\"Tipo1\", tipo)\n",
        "print(\"Tipo1\", tipo2)\n",
        "\n",
        "\n",
        "\n",
        "for i in range(len(X_train)):\n",
        "  img=X_train[i]\n",
        "  label=y_train[i]\n",
        "  if tipo== type(img):\n",
        "    imagep=preprocessing(img)\n",
        "    resized_image = cv2.resize(imagep, (32, 32))\n",
        "    X_trainp.append(resized_image)\n",
        "    y_trainp.append(label)\n",
        "\n",
        "for i in range(len(X_test)):\n",
        "  img2=X_test[i]\n",
        "  label2=y_test[i]\n",
        "  if tipo2== type(img2):\n",
        "    imagep2=preprocessing(img2)\n",
        "    resized_image2 = cv2.resize(imagep2, (32, 32))\n",
        "    X_testp.append(resized_image2)\n",
        "    y_testp.append(label2)\n",
        "\n",
        "print(\"After\")\n",
        "print(\"X_trainp: \",len(X_trainp))\n",
        "print(\"y_trainp: \",len(y_trainp))\n",
        "print(\"X_testp: \",len(X_testp))\n",
        "print(\"y_testp: \",len(y_testp))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1OhhKC6aypP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "outputId": "f2950614-0151-4e1c-c735-62d8a197a3e0"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-267-3ef9a3ad8140>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m############################### TRAIN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Create model structure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmyModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-5dd92f3f009c>\u001b[0m in \u001b[0;36mmyModel\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# TODO: Add layers as presented in the class to conform your CNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     model.compile(optimizer=tf.keras.optimizers.RMSprop(),                                                                                                                                                                       \n\u001b[0m\u001b[1;32m     16\u001b[0m               \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBinaryCrossentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m               metrics=[tf.keras.metrics.BinaryAccuracy(),                                                                                                                                                                        \n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "############################### TRAIN\n",
        "# Create model structure\n",
        "model = myModel()\n",
        "\n",
        "print(model.summary())\n",
        "# Train the model\n",
        "\n",
        "# TODO: Modify the ImageDataGenerator attributes to improve your neural network performance if necessary\n",
        "dataGen = ImageDataGenerator()\n",
        "\n",
        "history=model.fit_generator(dataGen.flow(X_train,y_train,batch_size=batch_size_val),epochs=epochs_val,validation_data=(X_validation,y_validation),shuffle=1)\n",
        "model.save('drive/MyDrive/AI_in_ROS/saved_model/my_model')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dLaxoAhbyZZ"
      },
      "source": [
        "Results \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMLtJ4FWb9_I"
      },
      "outputs": [],
      "source": [
        "############################### PLOT\n",
        "plt.figure()\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.legend(['training','validation'])\n",
        "plt.title('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()\n",
        "score =model.evaluate(X_test,y_test,verbose=0)\n",
        "print('Test Score:',score[0])\n",
        "print('Test Accuracy:',score[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OI0jZExpuwZT"
      },
      "outputs": [],
      "source": [
        "model = load_model('drive/MyDrive/AI_in_ROS/saved_model/my_model')\n",
        "\n",
        "random_index = random.randint(0, len(images))\n",
        "test_image = images[random_index]\n",
        "\n",
        "test_image = preprocessing(test_image)\n",
        "test_image = cv2.resize(test_image, (imageDimesions[0], imageDimesions[1]))\n",
        "test_image = tf.expand_dims(test_image, axis=0)\n",
        "\n",
        "test_image_class =classNo[random_index]\n",
        "\n",
        "test_image_class_prediction = model.predict(test_image)\n",
        "\n",
        "class_val = np.argmax(test_image_class_prediction[0], axis=0)\n",
        "\n",
        "print(\"IMAGE CLASS: {0}\".format(test_image_class))\n",
        "print(\"IMAGE PREDICTION: {0}\".format(class_val))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLxyqrS0uwZU"
      },
      "source": [
        "The following code can be used to test the model with your USB camera. You'll have to port it into ROS "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1OOPmnyecuG"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "tf.config.set_visible_devices([], 'GPU')\n",
        " \n",
        "#############################################\n",
        " \n",
        "frameWidth= 640         # CAMERA RESOLUTION\n",
        "frameHeight = 480\n",
        "brightness = 180\n",
        "threshold = 0.75         # PROBABLITY THRESHOLD\n",
        "font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "##############################################\n",
        " \n",
        "# SETUP THE VIDEO CAMERA\n",
        "cap = cv2.VideoCapture(0)\n",
        "cap.set(3, frameWidth)\n",
        "cap.set(4, frameHeight)\n",
        "cap.set(10, brightness)\n",
        "\n",
        "model = load_model('saved_model/my_model')\n",
        " \n",
        "def grayscale(img):\n",
        "    img = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
        "    return img\n",
        "def equalize(img):\n",
        "    img =cv2.equalizeHist(img)\n",
        "    return img\n",
        "def preprocessing(img):\n",
        "    img = grayscale(img)\n",
        "    img = equalize(img)\n",
        "    img = img/255\n",
        "    return img\n",
        "def getCalssName(classNo):\n",
        "    if   classNo == 0: return 'Speed Limit 20 km/h'\n",
        "    elif classNo == 1: return 'Speed Limit 30 km/h'\n",
        "    elif classNo == 2: return 'Speed Limit 50 km/h'\n",
        "    elif classNo == 3: return 'Speed Limit 60 km/h'\n",
        "    elif classNo == 4: return 'Speed Limit 70 km/h'\n",
        "    elif classNo == 5: return 'Speed Limit 80 km/h'\n",
        "    elif classNo == 6: return 'End of Speed Limit 80 km/h'\n",
        "    elif classNo == 7: return 'Speed Limit 100 km/h'\n",
        "    elif classNo == 8: return 'Speed Limit 120 km/h'\n",
        "    elif classNo == 9: return 'No passing'\n",
        "    elif classNo == 10: return 'No passing for vechiles over 3.5 metric tons'\n",
        "    elif classNo == 11: return 'Right-of-way at the next intersection'\n",
        "    elif classNo == 12: return 'Priority road'\n",
        "    elif classNo == 13: return 'Yield'\n",
        "    elif classNo == 14: return 'Stop'\n",
        "    elif classNo == 15: return 'No vechiles'\n",
        "    elif classNo == 16: return 'Vechiles over 3.5 metric tons prohibited'\n",
        "    elif classNo == 17: return 'No entry'\n",
        "    elif classNo == 18: return 'General caution'\n",
        "    elif classNo == 19: return 'Dangerous curve to the left'\n",
        "    elif classNo == 20: return 'Dangerous curve to the right'\n",
        "    elif classNo == 21: return 'Double curve'\n",
        "    elif classNo == 22: return 'Bumpy road'\n",
        "    elif classNo == 23: return 'Slippery road'\n",
        "    elif classNo == 24: return 'Road narrows on the right'\n",
        "    elif classNo == 25: return 'Road work'\n",
        "    elif classNo == 26: return 'Traffic signals'\n",
        "    elif classNo == 27: return 'Pedestrians'\n",
        "    elif classNo == 28: return 'Children crossing'\n",
        "    elif classNo == 29: return 'Bicycles crossing'\n",
        "    elif classNo == 30: return 'Beware of ice/snow'\n",
        "    elif classNo == 31: return 'Wild animals crossing'\n",
        "    elif classNo == 32: return 'End of all speed and passing limits'\n",
        "    elif classNo == 33: return 'Turn right ahead'\n",
        "    elif classNo == 34: return 'Turn left ahead'\n",
        "    elif classNo == 35: return 'Ahead only'\n",
        "    elif classNo == 36: return 'Go straight or right'\n",
        "    elif classNo == 37: return 'Go straight or left'\n",
        "    elif classNo == 38: return 'Keep right'\n",
        "    elif classNo == 39: return 'Keep left'\n",
        "    elif classNo == 40: return 'Roundabout mandatory'\n",
        "    elif classNo == 41: return 'End of no passing'\n",
        "    elif classNo == 42: return 'End of no passing by vechiles over 3.5 metric tons'\n",
        "     \n",
        "while True:\n",
        " \n",
        "    # READ IMAGE\n",
        "    success, imgOrignal = cap.read()\n",
        "         \n",
        "    # PROCESS IMAGE\n",
        "    img = np.asarray(imgOrignal)\n",
        "    img = cv2.resize(img, (32, 32))\n",
        "    img = preprocessing(img)\n",
        "\n",
        "    img = img.reshape(1, 32, 32, 1)\n",
        "    cv2.putText(imgOrignal, \"CLASS: \" , (20, 35), font, 0.75, (0, 0, 255), 2, cv2.LINE_AA)\n",
        "    cv2.putText(imgOrignal, \"PROBABILITY: \", (20, 75), font, 0.75, (0, 0, 255), 2, cv2.LINE_AA)\n",
        "    # PREDICT IMAGE\n",
        "    predictions = model.predict(img)\n",
        "    classIndex = model.predict_classes(img)\n",
        "    probabilityValue =np.amax(predictions)\n",
        "    if probabilityValue > threshold:\n",
        "       print(getCalssName(classIndex))\n",
        "       cv2.putText(imgOrignal,str(classIndex)+\" \"+str(getCalssName(classIndex)), (120, 35), font, 0.75, (0, 0, 255), 2, cv2.LINE_AA)\n",
        "       cv2.putText(imgOrignal, str(round(probabilityValue*100,2) )+\"%\", (180, 75), font, 0.75, (0, 0, 255), 2, cv2.LINE_AA)\n",
        "       cv2.imshow(\"Result\", imgOrignal)\n",
        "    else:\n",
        "        print('Not found')\n",
        "        cv2.putText(imgOrignal, 'No Traffic Sign', (120, 35), font, 0.75,\n",
        "                    (0, 0, 255), 2, cv2.LINE_AA)\n",
        "\n",
        "        cv2.imshow(\"Result\", imgOrignal)\n",
        "         \n",
        "    if cv2.waitKey(1) and 0xFF == ord('q'):\n",
        "       break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eOSMi4YyVO8g"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "model = load_model('drive/MyDrive/AI_in_ROS/saved_model/my_model')\n",
        "print(model.summary())\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "AI_in_ROS.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}