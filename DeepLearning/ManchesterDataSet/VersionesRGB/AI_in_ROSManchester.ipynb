{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_8W44vyaZUcd"
      },
      "outputs": [],
      "source": [
        "# Main requirements \n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from tensorflow.keras import models, layers \n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "import pandas as pd\n",
        "import random\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import models, layers, optimizers\n",
        "\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Flatten\n",
        "from keras.constraints import maxnorm\n",
        "#from keras.optimizers import SGD\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.convolutional import MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "from keras import backend as K\n",
        "from tensorflow.keras.applications import VGG16\n",
        "#K.set_image_dim_ordering('th')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTyDk_q9a7qq"
      },
      "source": [
        "BEG FUNTION DEF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "a55HfuchbBlJ"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "def grayscale(img):\n",
        "    img = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
        "    return img\n",
        "\"\"\"\n",
        "def equalize(img):\n",
        "    image_hsv=np.copy(img)\n",
        "    image_hsv=cv2.cvtColor(image_hsv, cv2.COLOR_RGB2HSV)\n",
        "    H,S,V=cv2.split(image_hsv)\n",
        "    eq_S=cv2.equalizeHist(S)\n",
        "    eq_V=cv2.equalizeHist(V)\n",
        "    eq_H=cv2.equalizeHist(H)\n",
        "    image_eq_hsv=cv2.merge([H, eq_S, eq_V])\n",
        "    img=cv2.cvtColor(image_eq_hsv, cv2.COLOR_HSV2RGB)\n",
        "    return img\n",
        "def preprocessing(img):\n",
        "    #img = grayscale(img)     # CONVERT TO GRAYSCALE\n",
        "    img = equalize(img)      # STANDARDIZE THE LIGHTING IN AN IMAGE\n",
        "    img = img/255            # TO NORMALIZE VALUES BETWEEN 0 AND 1 INSTEAD OF 0 TO 255\n",
        "    return img\n",
        "\n",
        "#Reshape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Uap38SeXbVU8"
      },
      "outputs": [],
      "source": [
        "def myModel():\n",
        "    # Hyperparameter selection                                                                                                                                                                                                                   \n",
        "    # Filters of the CNN                                                                                                                                                                                                         \n",
        "    no_Of_Filters=60                                                                                                                                                                                                             \n",
        "    # Shape of the filters used in the CNN                                                                                                                                                                                       \n",
        "    size_of_Filter=(5,5)                                                                                                                                                                                                         \n",
        "    size_of_Filter2=(3,3)                                                                                                                                                                                                        \n",
        "    # Tekes batches of 2x2 pixels and avg the                                                                                                                                                                                    \n",
        "    size_of_pool=(2,2)                                                                                                                                                                                                           \n",
        "    # Nodes of the neural classifier                                                                                                                                                                                             \n",
        "    no_Of_Nodes = 500                                                                                                                                                                                                            \n",
        "                                                                                                                                                                                                                                 \n",
        "    # TODO: Add layers as presented in the class to conform your CNN \n",
        "    model = Sequential()                                                                                                                                                                                                                             \n",
        "    model.add(Conv2D(32, (3, 3), input_shape=(3, 32, 32), padding='same', activation='relu', kernel_constraint=maxnorm(3)))                                                                                                                                                              \n",
        "    model.compile(optimizer=tf.keras.optimizers.RMSprop(),                                                                                                                                                                       \n",
        "              loss=tf.keras.losses.BinaryCrossentropy(),                                                                                                                                                                         \n",
        "              metrics=[tf.keras.metrics.BinaryAccuracy(),                                                                                                                                                                        \n",
        "                       tf.keras.metrics.FalseNegatives()])                                                                                                                                                                       \n",
        "                                                                                                                                                                                                                                 \n",
        "    return model                                                                                                                                                                                                                 "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_gen = ImageDataGenerator(\n",
        "validation_split=0.2,\n",
        "preprocessing_function=preprocessing)\n",
        "\n",
        "# Standardize the color of each channel\n",
        "test_data_gen = ImageDataGenerator(rescale=1./255, preprocessing_function=preprocessing)\n",
        "\n",
        "# Creates a train data generator and a validation data generator from the train images using a split of 80-20%\n",
        "# Each image will be 150x150, and the generator will output batches of 20 tensors\n",
        "train_generator = train_data_gen.flow_from_directory(\n",
        "'drive/Shareddrives/TE3002B/AI_in_ROS/MyData',\n",
        "target_size=(28, 28),\n",
        "batch_size=20,\n",
        "class_mode='binary',\n",
        "subset='training')\n",
        "\n",
        "validation_generator = train_data_gen.flow_from_directory(\n",
        "'drive/Shareddrives/TE3002B/AI_in_ROS/MyData',\n",
        "target_size=(28, 28),\n",
        "batch_size=20,\n",
        "class_mode='binary',\n",
        "subset='validation')\n",
        "\n",
        "# Image data generator for the test data: it is processed the same as the train data\n",
        "test_generator = test_data_gen.flow_from_directory(\n",
        "'drive/Shareddrives/TE3002B/AI_in_ROS/MyData',\n",
        "target_size=(28, 28),\n",
        "batch_size=20,\n",
        "class_mode='binary')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLnLahw0HtvC",
        "outputId": "ba000db0-9c89-4575-a6e2-db348cd52a7b"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 21312 images belonging to 43 classes.\n",
            "Found 5328 images belonging to 43 classes.\n",
            "Found 26640 images belonging to 43 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "augmented_data_gen = ImageDataGenerator(\n",
        "rescale=1./255,\n",
        "\n",
        "rotation_range = 40,\n",
        "width_shift_range = 0.2,\n",
        "height_shift_range = 0.2,\n",
        "shear_range = 0.2,\n",
        "zoom_range=0.2,\n",
        "horizontal_flip=True,\n",
        "\n",
        "validation_split=0.2,\n",
        "fill_mode='nearest',\n",
        "preprocessing_function=preprocessing)"
      ],
      "metadata": {
        "id": "uC-DDWosIyTY"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = 'drive/Shareddrives/TE3002B/AI_in_ROS/MyData/14'\n",
        "\n",
        "fnames = [os.path.join(train_dir, fname) for fname in os.listdir(train_dir)]\n",
        "\n",
        "# We pick one image to \"augment\"\n",
        "img_path = fnames[4]\n",
        "print(\"TypeImage :\",type(img_path))\n",
        "print(img_path)\n",
        "\n",
        "image = cv2.imread(img_path)\n",
        "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "image = cv2.resize(image, dsize=(28, 28), interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "plt.figure(figsize=(15,10))\n",
        "\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.title(\"Original\")\n",
        "plt.imshow(image)\n",
        "# Read the image and resize it\n",
        "\n",
        "img_tensor = np.expand_dims(image, axis=0)\n",
        "print(img_tensor.shape)\n",
        "\n",
        "idx = 1\n",
        "for batch in augmented_data_gen.flow(img_tensor, batch_size=1):\n",
        "  plt.subplot(2, 2, idx + 1)\n",
        "  plt.title(\"Transform:\" + str(idx))\n",
        "  plt.imshow(batch[0])\n",
        "  idx += 1\n",
        "  if idx % 4 == 0:\n",
        "    break\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 728
        },
        "id": "1z2u_uEdI19W",
        "outputId": "62ac5a79-e91d-4f65-d1bd-d4751570c74b"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TypeImage : <class 'str'>\n",
            "drive/Shareddrives/TE3002B/AI_in_ROS/MyData/14/00000_00025.ppm\n",
            "(1, 28, 28, 3)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "error",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-70-53aa613ff036>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maugmented_data_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Transform:\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras_preprocessing/image/iterator.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras_preprocessing/image/iterator.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;31m# The transformation of images is not under thread lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;31m# so it can be done in parallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_batches_of_transformed_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_batches_of_transformed_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras_preprocessing/image/numpy_array_iterator.py\u001b[0m in \u001b[0;36m_get_batches_of_transformed_samples\u001b[0;34m(self, index_array)\u001b[0m\n\u001b[1;32m    161\u001b[0m             x = self.image_data_generator.apply_transform(\n\u001b[1;32m    162\u001b[0m                 x.astype(self.dtype), params)\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_data_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstandardize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m             \u001b[0mbatch_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras_preprocessing/image/image_data_generator.py\u001b[0m in \u001b[0;36mstandardize\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    706\u001b[0m         \"\"\"\n\u001b[1;32m    707\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing_function\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrescale\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrescale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-805488a8abd9>\u001b[0m in \u001b[0;36mpreprocessing\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m#img = grayscale(img)     # CONVERT TO GRAYSCALE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mequalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m      \u001b[0;31m# STANDARDIZE THE LIGHTING IN AN IMAGE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m            \u001b[0;31m# TO NORMALIZE VALUES BETWEEN 0 AND 1 INSTEAD OF 0 TO 255\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-805488a8abd9>\u001b[0m in \u001b[0;36mequalize\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mimage_hsv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_hsv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_RGB2HSV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_hsv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0meq_S\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequalizeHist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0meq_V\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequalizeHist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0meq_H\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequalizeHist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31merror\u001b[0m: OpenCV(4.1.2) /io/opencv/modules/imgproc/src/histogram.cpp:3429: error: (-215:Assertion failed) _src.type() == CV_8UC1 in function 'equalizeHist'\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x720 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oG2ZROdFa-6p"
      },
      "source": [
        "\n",
        "\n",
        "END FUNTION DEF\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1r_cr0nHmclS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20eb9271-4735-4a21-cbfa-ff127d222eee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "kmxNwbREagzL"
      },
      "outputs": [],
      "source": [
        "################# Parameters #####################\n",
        "tf.config.set_visible_devices([], 'GPU')\n",
        "\n",
        "path = \"drive/Shareddrives/TE3002B/AI_in_ROS/MyData\" # folder with all the class folders\n",
        "labelFile = 'drive/Shareddrives/TE3002B/AI_in_ROS/labels.csv' # file with all names of classes\n",
        "batch_size_val=50  # how many to process together before updating the interanl parameters\n",
        "steps_per_epoch_val=100 # we divide all our database in 10 bathces \n",
        "epochs_val=10\n",
        "imageDimesions = (32,32,3)\n",
        "testRatio = 0.2    # if 1000 images split will 200 for testing\n",
        "validationRatio = 0.2 # if 1000 images 20% of remaining 800 will be 160 for validation\n",
        "###################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "mZUYQZ8bai10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebaeafaf-640f-4570-a119-6d8032af2624"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Classes Detected: 43\n",
            "Importing Classes.....\n",
            "0 of 43\n",
            "1 of 43\n",
            "2 of 43\n",
            "3 of 43\n",
            "4 of 43\n",
            "5 of 43\n",
            "6 of 43\n",
            "7 of 43\n",
            "8 of 43\n",
            "9 of 43\n",
            "10 of 43\n",
            "11 of 43\n",
            "12 of 43\n",
            "13 of 43\n",
            "14 of 43\n",
            "15 of 43\n",
            "16 of 43\n",
            "17 of 43\n",
            "18 of 43\n",
            "19 of 43\n",
            "20 of 43\n",
            "21 of 43\n",
            "22 of 43\n",
            "23 of 43\n",
            "24 of 43\n",
            "25 of 43\n",
            "26 of 43\n",
            "27 of 43\n",
            "28 of 43\n",
            "29 of 43\n",
            "30 of 43\n",
            "31 of 43\n",
            "32 of 43\n",
            "33 of 43\n",
            "34 of 43\n",
            "35 of 43\n",
            "36 of 43\n",
            "37 of 43\n",
            "38 of 43\n",
            "39 of 43\n",
            "40 of 43\n",
            "41 of 43\n",
            "42 of 43\n",
            " \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:20: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
          ]
        }
      ],
      "source": [
        "############################### Importing of the Images\n",
        "count = 0\n",
        "images = []\n",
        "classNo = []\n",
        "myList = os.listdir(path)\n",
        "print(\"Total Classes Detected:\",len(myList))\n",
        "noOfClasses=len(myList)\n",
        "print(\"Importing Classes.....\")\n",
        "\n",
        "#Import names\n",
        "for x in range (0,len(myList)):\n",
        "    myPicList = os.listdir(path+\"/\"+str(count))\n",
        "    for y in myPicList:\n",
        "        curImg = cv2.imread(path+\"/\"+str(count)+\"/\"+y)\n",
        "        images.append(curImg)\n",
        "        classNo.append(count)\n",
        "    print(\"{0} of {1}\".format(count, len(myList)), end =\"\\n\")\n",
        "    count +=1\n",
        "print(\" \")\n",
        "images = np.array(images)\n",
        "classNo = np.array(classNo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "OzgYO4j7al5Z"
      },
      "outputs": [],
      "source": [
        "############################### Split Data\n",
        "X_train, X_test, y_train, y_test = train_test_split(images, classNo, test_size=testRatio)\n",
        "X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=validationRatio)\n",
        " \n",
        "# X_train = ARRAY OF IMAGES TO TRAIN\n",
        "# y_train = CORRESPONDING CLASS ID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "PsP8ythkas1p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6c47cdf-9e9b-4e8c-aa59-f1b422427aee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data shape  (43, 2) <class 'pandas.core.frame.DataFrame'>\n",
            "17076\n",
            "5337\n",
            "43 2\n"
          ]
        }
      ],
      "source": [
        "############################### READ CSV FILE\n",
        "data=pd.read_csv(labelFile)\n",
        "print(\"data shape \",data.shape,type(data))\n",
        "\n",
        "###################################################3LF\n",
        "print(len(X_train))\n",
        "print(len(X_test))\n",
        "#print(len(label_file), label_file.shape)\n",
        "#print(label_file[:10])\n",
        "#####\n",
        "H,W=data.shape\n",
        "print(H,W)\n",
        "#data = data.reshape((17076, 55 * 57 *3))\n",
        "#data = data.astype('float32') / 255)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-53lOEjlavpG"
      },
      "outputs": [],
      "source": [
        "############################### DISPLAY SOME SAMPLES IMAGES  OF ALL THE CLASSES\n",
        "num_of_samples = []\n",
        "cols = 5\n",
        "num_classes = noOfClasses\n",
        "fig, axs = plt.subplots(nrows=num_classes, ncols=cols, figsize=(5, 300))\n",
        "fig.tight_layout()\n",
        "for i in range(cols):\n",
        "    for j,row in data.iterrows():\n",
        "        x_selected = X_train[y_train == j]\n",
        "        axs[j][i].imshow(x_selected[random.randint(0, len(x_selected)- 1)], cmap=plt.get_cmap(\"gray\"))\n",
        "        axs[j][i].axis(\"off\")\n",
        "        if i == 2:\n",
        "            axs[j][i].set_title(str(j)+ \"-\"+row[\"Name\"])\n",
        "            num_of_samples.append(len(x_selected))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "LqVzTDkbaxOl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "d01239a7-b0ea-4a77-d983-40fc561b9640"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[88, 982, 975, 620, 885, 852, 176, 628, 606, 633, 850, 546, 887, 918, 347, 282, 187, 461, 542, 95, 157, 158, 173, 210, 109, 634, 257, 104, 248, 115, 203, 350, 120, 299, 172, 514, 177, 99, 885, 139, 162, 118, 113]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "############################### DISPLAY A BAR CHART SHOWING NO OF SAMPLES FOR EACH CATEGORY\n",
        "print(num_of_samples)\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.bar(range(0, num_classes), num_of_samples)\n",
        "plt.title(\"Distribution of the training dataset\")\n",
        "plt.xlabel(\"Class number\")\n",
        "plt.ylabel(\"Number of images\")\n",
        "plt.show()\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(images, classNo, test_size=testRatio)\n",
        "X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=validationRatio)"
      ],
      "metadata": {
        "id": "QXPNW2TrJ7kN"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "YCArJGJluwZQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e932cf5c-66be-430b-988a-a451224de1f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before\n",
            "Len X_train:  17076\n",
            "X_train[1]:  (49, 49, 3)\n",
            "X_train shape: (17076,)\n",
            "y_train shape: (17076,)\n",
            "X_test shape: (5337,)\n",
            "y_test shape: (5337,)\n",
            "#####################################################\n",
            "#####################################################\n",
            "Tipo1 <class 'numpy.ndarray'>\n",
            "Tipo1 <class 'numpy.ndarray'>\n",
            "After\n",
            "X_trainp:  17050\n",
            "y_trainp:  17050\n",
            "X_testp:  5322\n",
            "y_testp:  5322\n",
            "Before\n",
            "<class 'list'>\n",
            "<class 'list'>\n",
            "<class 'list'>\n",
            "<class 'list'>\n",
            "After\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "Len:  17050\n",
            "Len:  5322\n",
            "Before\n",
            "Train images shape: (17050, 28, 28, 3)\n",
            "Train test images shape: (5322, 28, 28, 3)\n",
            "After\n",
            "Train images shape: (17050, 28, 28, 3)\n",
            "Test images shape: (5322, 28, 28, 3)\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.core.framework.types_pb2 import DataType\n",
        "############################### Split Data\n",
        "\n",
        " \n",
        "# X_train = ARRAY OF IMAGES TO TRAIN\n",
        "# y_train = CORRESPONDING CLASS ID\n",
        "\n",
        "# TODO: Adapt the X_train, X_test, X_validation, y_train, y_test and y_validation arrays for the proper neural network training, validation and testing\n",
        "#lf\n",
        "\n",
        "\n",
        "print(\"Before\")\n",
        "print(\"Len X_train: \",len(X_train))\n",
        "print(\"X_train[1]: \",X_train[1].shape)\n",
        "#print(\"y_train[1]: \",y_train.shape)\n",
        "print(f'X_train shape: {X_train.shape}')\n",
        "print(f'y_train shape: {y_train.shape}')\n",
        "print(f'X_test shape: {X_test.shape}')\n",
        "print(f'y_test shape: {y_test.shape}')\n",
        "\n",
        "print(\"#####################################################\")\n",
        "\n",
        "X_trainp=[]\n",
        "X_testp=[]\n",
        "y_trainp=[]\n",
        "y_testp=[]\n",
        "\n",
        "print(\"#####################################################\")\n",
        "\n",
        "tipo=type(X_train[1])\n",
        "tipo2=type(X_test[1])\n",
        "print(\"Tipo1\", tipo)\n",
        "print(\"Tipo1\", tipo2)\n",
        "\n",
        "\n",
        "\n",
        "for i in range(len(X_train)):\n",
        "  img=X_train[i]\n",
        "  label=y_train[i]\n",
        "  if tipo== type(img):\n",
        "    imagep=preprocessing(img)\n",
        "    resized_image = cv2.resize(imagep, (28, 28))\n",
        "    X_trainp.append(resized_image)\n",
        "    y_trainp.append(label)\n",
        "\n",
        "for i in range(len(X_test)):\n",
        "  img2=X_test[i]\n",
        "  label2=y_test[i]\n",
        "  if tipo2== type(img2):\n",
        "    imagep2=preprocessing(img2)\n",
        "    resized_image2 = cv2.resize(imagep2, (28, 28))\n",
        "    X_testp.append(resized_image2)\n",
        "    y_testp.append(label2)\n",
        "\n",
        "print(\"After\")\n",
        "print(\"X_trainp: \",len(X_trainp))\n",
        "print(\"y_trainp: \",len(y_trainp))\n",
        "print(\"X_testp: \",len(X_testp))\n",
        "print(\"y_testp: \",len(y_testp))\n",
        "print(\"Before\")\n",
        "print(type(X_trainp))\n",
        "print(type(y_trainp))\n",
        "print(type(X_testp))\n",
        "print(type(y_testp))\n",
        "print(\"After\")\n",
        "#############################################\n",
        "train_images = np.array(X_trainp)\n",
        "train_labels = np.array(y_trainp)\n",
        "test_images  = np.array(X_testp)\n",
        "test_labels  = np.array(y_testp)\n",
        "###############################################\n",
        "print(type(train_images))\n",
        "print(type(train_labels))\n",
        "print(type(test_images))\n",
        "print(type(test_labels))\n",
        "\n",
        "\n",
        "#####################################################################\n",
        "###Reshape####\n",
        "print(\"Len: \", len(train_images))\n",
        "train_imagesLEN=len(train_images)\n",
        "test_imagesLEN=len(test_images)\n",
        "print(\"Len: \", len(test_images))\n",
        "\n",
        "print(\"Before\")\n",
        "print(f'Train images shape: {train_images.shape}')\n",
        "print(f'Train test images shape: {test_images.shape}')\n",
        "train_images = train_images.reshape((train_imagesLEN, 28 , 28 , 3))\n",
        "test_images = test_images.reshape((test_imagesLEN, 28 , 28 , 3))\n",
        "print('After')\n",
        "print(f'Train images shape: {train_images.shape}')\n",
        "\n",
        "# turn values from 0-255 to 0-1\n",
        "#train_images = train_images.astype('float32') / 255 \n",
        "\n",
        "# same starndadization for the test images\n",
        "#test_images = test_images.reshape((test_imagesLEN, 32 * 32)) \n",
        "#test_images = test_images.astype('float32') / 255\n",
        "print(f'Test images shape: {test_images.shape}')\n",
        "\n",
        "\n",
        "train_labels = to_categorical(train_labels) \n",
        "test_labels = to_categorical(test_labels)\n",
        "print(train_labels[1])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################################################################3333"
      ],
      "metadata": {
        "id": "EBLntdAOgCrO"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.Sequential()\n",
        "\n",
        "model.add(layers.Conv2D(60, (3,3), padding='same', strides=1, activation='relu', input_shape=(28, 28, 3)))\n",
        "model.add(layers.Conv2D(60, (5,5), padding='valid', strides=1, activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2,2)))\n",
        "\n",
        "model.add(layers.Conv2D(30, (3,3), padding='valid', strides=1, activation='relu'))\n",
        "model.add(layers.Conv2D(30, (3,3), padding='valid', strides=1, activation='relu'))\n",
        "\n",
        "model.add(layers.MaxPooling2D((2,2)))\n",
        "\n",
        "model.add(Dropout(0.5))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(500, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(layers.Dense(43, activation='softmax'))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0VESEKEqg-Aw",
        "outputId": "53da50e1-fa11-406f-e1e9-818a6944f474"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_37 (Conv2D)          (None, 28, 28, 60)        1680      \n",
            "                                                                 \n",
            " conv2d_38 (Conv2D)          (None, 24, 24, 60)        90060     \n",
            "                                                                 \n",
            " max_pooling2d_18 (MaxPoolin  (None, 12, 12, 60)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_39 (Conv2D)          (None, 10, 10, 30)        16230     \n",
            "                                                                 \n",
            " conv2d_40 (Conv2D)          (None, 8, 8, 30)          8130      \n",
            "                                                                 \n",
            " max_pooling2d_19 (MaxPoolin  (None, 4, 4, 30)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_14 (Dropout)        (None, 4, 4, 30)          0         \n",
            "                                                                 \n",
            " flatten_7 (Flatten)         (None, 480)               0         \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 500)               240500    \n",
            "                                                                 \n",
            " dropout_15 (Dropout)        (None, 500)               0         \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 43)                21543     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 378,143\n",
            "Trainable params: 378,143\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "train_images = np.array(X_trainp)\n",
        "train_labels = np.array(y_trainp)\n",
        "test_images  = np.array(X_testp)\n",
        "test_labels  = np.array(y_testp)\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "model.fit(train_images, train_labels, epochs=5, validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxPrlJ7kh8L8",
        "outputId": "bdd81d2d-ca67-46b8-d0db-44b650149cf0"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "427/427 [==============================] - 194s 449ms/step - loss: 1.9888 - accuracy: 0.4282 - val_loss: 0.8273 - val_accuracy: 0.7135\n",
            "Epoch 2/5\n",
            "427/427 [==============================] - 184s 432ms/step - loss: 0.7098 - accuracy: 0.7699 - val_loss: 0.3453 - val_accuracy: 0.8997\n",
            "Epoch 3/5\n",
            "427/427 [==============================] - 172s 404ms/step - loss: 0.4155 - accuracy: 0.8690 - val_loss: 0.2104 - val_accuracy: 0.9425\n",
            "Epoch 4/5\n",
            "427/427 [==============================] - 171s 401ms/step - loss: 0.3186 - accuracy: 0.8974 - val_loss: 0.1648 - val_accuracy: 0.9557\n",
            "Epoch 5/5\n",
            "427/427 [==============================] - 183s 429ms/step - loss: 0.2484 - accuracy: 0.9205 - val_loss: 0.1671 - val_accuracy: 0.9572\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f910027a090>"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_images, test_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbBiEG62gFyV",
        "outputId": "93ee85e1-4b33-4ff2-ab34-30cc32cce3d2"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "167/167 [==============================] - 17s 103ms/step - loss: 0.1339 - accuracy: 0.9566\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.1339171975851059, 0.956595242023468]"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DRIVE_PATH = \"/content/drive/MyDrive/Colab\"\n",
        "MODEL_CHECKPOINTS = \"Ecualizacin1\"\n",
        "MODEL_SAVE_DIRECTORY = os.path.join(DRIVE_PATH,MODEL_CHECKPOINTS)"
      ],
      "metadata": {
        "id": "aY5DyEI5WD7e"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(MODEL_SAVE_DIRECTORY)\n",
        "os.path.exists(MODEL_SAVE_DIRECTORY)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCua9k6bF3po",
        "outputId": "cbece652-9bad-47b6-ce33-4327bb973213"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab/Ecualizacin1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(os.path.join(MODEL_SAVE_DIRECTORY,\"puzzlebot_model.h5\"))"
      ],
      "metadata": {
        "id": "VJP-6LIgGl-M"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(os.path.join(MODEL_SAVE_DIRECTORY,\"puzzlebot_model.pb\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFCzgTgiGpWr",
        "outputId": "262badc2-9c3d-450f-b469-b9b667a8c27d"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Colab/Ecualizacin1/puzzlebot_model.pb/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The keras.models.Sequential class is a wrapper for the neural network model that treats \n",
        "# the network as a sequence of layers\n",
        "IMG_INPUT_SHAPE = (32,32,3)\n",
        "convolutional =  VGG16(weights='imagenet', include_top = False, input_shape=IMG_INPUT_SHAPE)\n",
        "convolutional.summary()\n",
        "\n",
        "\n",
        "# Once we have our model built, we need to compile it before it can be run. \n",
        "# Compiling the Keras model calls the backend (tensorflow, theano, etc.) and binds the optimizer, \n",
        "# loss function, and other parameters required before the model can be run on any input data.\n",
        "\n",
        "# loss function: basically, the error function. categorical crossentropy is one of many.\n",
        "\n",
        "\n",
        "# optimizer: this is the mechanism through which the network will update itself.\n",
        "# Stochastic Gradient descent (sgd) is one of those.\n",
        "\n",
        "# Metrics to monitor during training and testing. Here we will only care about accuracy.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BQ6R3Jz36Gi",
        "outputId": "60609de4-3a6f-4cae-e46c-5894740955f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_4 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 32, 32, 64)        36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 16, 16, 64)        0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 16, 16, 128)       147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 8, 8, 256)         295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 8, 8, 256)         590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 8, 8, 256)         590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 4, 4, 256)         0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 4, 4, 512)         1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 4, 4, 512)         2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 4, 4, 512)         2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 2, 2, 512)         0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 2, 2, 512)         2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 2, 2, 512)         2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 2, 2, 512)         2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 1, 1, 512)         0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 14,714,688\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = ModelCheckpoint(filepath=\"mnist_weights.hdf5\", verbose=1, save_best_only=True)\n",
        "model.fit(train_data, train_labels, epochs=5, validation_split=0.2, callbacks=[checkpoint])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "id": "k_NnUI6ycSWi",
        "outputId": "70fe2c42-20a5-484c-a298-51c9d70ba327"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-f22b185eda86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mnist_weights.hdf5\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'ModelCheckpoint' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "network.fit(train_images, train_labels, epochs=50)\n",
        "#network.fit(train_images, train_labels, epochs=50,validation_split=0.2)\n",
        "\n",
        "test_loss, test_acc = network.evaluate(test_images, test_labels)\n",
        "\n",
        "print(\"test loss: \", test_loss, \"test accuracy: \", test_acc)"
      ],
      "metadata": {
        "id": "pM1iS6et4Dvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1OhhKC6aypP"
      },
      "outputs": [],
      "source": [
        "############################### TRAIN\n",
        "# Create model structure\n",
        "model = myModel()\n",
        "print(model.summary())\n",
        "# Train the model\n",
        "\n",
        "# TODO: Modify the ImageDataGenerator attributes to improve your neural network performance if necessary\n",
        "dataGen = ImageDataGenerator()\n",
        "\n",
        "history=model.fit_generator(dataGen.flow(X_train,y_train,batch_size=batch_size_val),epochs=epochs_val,validation_data=(X_validation,y_validation),shuffle=1)\n",
        "model.save('drive/MyDrive/AI_in_ROS/saved_model/my_model')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dLaxoAhbyZZ"
      },
      "source": [
        "Results \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMLtJ4FWb9_I"
      },
      "outputs": [],
      "source": [
        "############################### PLOT\n",
        "plt.figure()\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.legend(['training','validation'])\n",
        "plt.title('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()\n",
        "score =model.evaluate(X_test,y_test,verbose=0)\n",
        "print('Test Score:',score[0])\n",
        "print('Test Accuracy:',score[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OI0jZExpuwZT"
      },
      "outputs": [],
      "source": [
        "model = load_model('drive/MyDrive/AI_in_ROS/saved_model/my_model')\n",
        "\n",
        "random_index = random.randint(0, len(images))\n",
        "test_image = images[random_index]\n",
        "\n",
        "test_image = preprocessing(test_image)\n",
        "test_image = cv2.resize(test_image, (imageDimesions[0], imageDimesions[1]))\n",
        "test_image = tf.expand_dims(test_image, axis=0)\n",
        "\n",
        "test_image_class =classNo[random_index]\n",
        "\n",
        "test_image_class_prediction = model.predict(test_image)\n",
        "\n",
        "class_val = np.argmax(test_image_class_prediction[0], axis=0)\n",
        "\n",
        "print(\"IMAGE CLASS: {0}\".format(test_image_class))\n",
        "print(\"IMAGE PREDICTION: {0}\".format(class_val))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLxyqrS0uwZU"
      },
      "source": [
        "The following code can be used to test the model with your USB camera. You'll have to port it into ROS "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1OOPmnyecuG"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "tf.config.set_visible_devices([], 'GPU')\n",
        " \n",
        "#############################################\n",
        " \n",
        "frameWidth= 640         # CAMERA RESOLUTION\n",
        "frameHeight = 480\n",
        "brightness = 180\n",
        "threshold = 0.75         # PROBABLITY THRESHOLD\n",
        "font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "##############################################\n",
        " \n",
        "# SETUP THE VIDEO CAMERA\n",
        "cap = cv2.VideoCapture(0)\n",
        "cap.set(3, frameWidth)\n",
        "cap.set(4, frameHeight)\n",
        "cap.set(10, brightness)\n",
        "\n",
        "model = load_model('saved_model/my_model')\n",
        " \n",
        "def grayscale(img):\n",
        "    img = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
        "    return img\n",
        "def equalize(img):\n",
        "    img =cv2.equalizeHist(img)\n",
        "    return img\n",
        "def preprocessing(img):\n",
        "    img = grayscale(img)\n",
        "    img = equalize(img)\n",
        "    img = img/255\n",
        "    return img\n",
        "def getCalssName(classNo):\n",
        "    if   classNo == 0: return 'Speed Limit 20 km/h'\n",
        "    elif classNo == 1: return 'Speed Limit 30 km/h'\n",
        "    elif classNo == 2: return 'Speed Limit 50 km/h'\n",
        "    elif classNo == 3: return 'Speed Limit 60 km/h'\n",
        "    elif classNo == 4: return 'Speed Limit 70 km/h'\n",
        "    elif classNo == 5: return 'Speed Limit 80 km/h'\n",
        "    elif classNo == 6: return 'End of Speed Limit 80 km/h'\n",
        "    elif classNo == 7: return 'Speed Limit 100 km/h'\n",
        "    elif classNo == 8: return 'Speed Limit 120 km/h'\n",
        "    elif classNo == 9: return 'No passing'\n",
        "    elif classNo == 10: return 'No passing for vechiles over 3.5 metric tons'\n",
        "    elif classNo == 11: return 'Right-of-way at the next intersection'\n",
        "    elif classNo == 12: return 'Priority road'\n",
        "    elif classNo == 13: return 'Yield'\n",
        "    elif classNo == 14: return 'Stop'\n",
        "    elif classNo == 15: return 'No vechiles'\n",
        "    elif classNo == 16: return 'Vechiles over 3.5 metric tons prohibited'\n",
        "    elif classNo == 17: return 'No entry'\n",
        "    elif classNo == 18: return 'General caution'\n",
        "    elif classNo == 19: return 'Dangerous curve to the left'\n",
        "    elif classNo == 20: return 'Dangerous curve to the right'\n",
        "    elif classNo == 21: return 'Double curve'\n",
        "    elif classNo == 22: return 'Bumpy road'\n",
        "    elif classNo == 23: return 'Slippery road'\n",
        "    elif classNo == 24: return 'Road narrows on the right'\n",
        "    elif classNo == 25: return 'Road work'\n",
        "    elif classNo == 26: return 'Traffic signals'\n",
        "    elif classNo == 27: return 'Pedestrians'\n",
        "    elif classNo == 28: return 'Children crossing'\n",
        "    elif classNo == 29: return 'Bicycles crossing'\n",
        "    elif classNo == 30: return 'Beware of ice/snow'\n",
        "    elif classNo == 31: return 'Wild animals crossing'\n",
        "    elif classNo == 32: return 'End of all speed and passing limits'\n",
        "    elif classNo == 33: return 'Turn right ahead'\n",
        "    elif classNo == 34: return 'Turn left ahead'\n",
        "    elif classNo == 35: return 'Ahead only'\n",
        "    elif classNo == 36: return 'Go straight or right'\n",
        "    elif classNo == 37: return 'Go straight or left'\n",
        "    elif classNo == 38: return 'Keep right'\n",
        "    elif classNo == 39: return 'Keep left'\n",
        "    elif classNo == 40: return 'Roundabout mandatory'\n",
        "    elif classNo == 41: return 'End of no passing'\n",
        "    elif classNo == 42: return 'End of no passing by vechiles over 3.5 metric tons'\n",
        "     \n",
        "while True:\n",
        " \n",
        "    # READ IMAGE\n",
        "    success, imgOrignal = cap.read()\n",
        "         \n",
        "    # PROCESS IMAGE\n",
        "    img = np.asarray(imgOrignal)\n",
        "    img = cv2.resize(img, (32, 32))\n",
        "    img = preprocessing(img)\n",
        "\n",
        "    img = img.reshape(1, 32, 32, 1)\n",
        "    cv2.putText(imgOrignal, \"CLASS: \" , (20, 35), font, 0.75, (0, 0, 255), 2, cv2.LINE_AA)\n",
        "    cv2.putText(imgOrignal, \"PROBABILITY: \", (20, 75), font, 0.75, (0, 0, 255), 2, cv2.LINE_AA)\n",
        "    # PREDICT IMAGE\n",
        "    predictions = model.predict(img)\n",
        "    classIndex = model.predict_classes(img)\n",
        "    probabilityValue =np.amax(predictions)\n",
        "    if probabilityValue > threshold:\n",
        "       print(getCalssName(classIndex))\n",
        "       cv2.putText(imgOrignal,str(classIndex)+\" \"+str(getCalssName(classIndex)), (120, 35), font, 0.75, (0, 0, 255), 2, cv2.LINE_AA)\n",
        "       cv2.putText(imgOrignal, str(round(probabilityValue*100,2) )+\"%\", (180, 75), font, 0.75, (0, 0, 255), 2, cv2.LINE_AA)\n",
        "       cv2.imshow(\"Result\", imgOrignal)\n",
        "    else:\n",
        "        print('Not found')\n",
        "        cv2.putText(imgOrignal, 'No Traffic Sign', (120, 35), font, 0.75,\n",
        "                    (0, 0, 255), 2, cv2.LINE_AA)\n",
        "\n",
        "        cv2.imshow(\"Result\", imgOrignal)\n",
        "         \n",
        "    if cv2.waitKey(1) and 0xFF == ord('q'):\n",
        "       break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eOSMi4YyVO8g"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "model = load_model('drive/MyDrive/AI_in_ROS/saved_model/my_model')\n",
        "print(model.summary())\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "AI_in_ROSManchester.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}